<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes | Brian Hengen's AI Experiments</title><meta name=keywords content="AI,SLM,Fine-tuning,Granite,Management AI,Executive Assistant"><meta name=description content="How I fine-tuned IBM Granite 4.0 H Micro on 5+ years of management notes to create a personalized AI assistant—after three failures, the key was data quality."><meta name=author content="Brian Hengen"><link rel=canonical href=https://brianhengen.us/posts/fine-tuning-management-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://brianhengen.us/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://brianhengen.us/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://brianhengen.us/favicon-32x32.png><link rel=apple-touch-icon href=https://brianhengen.us/apple-touch-icon.png><link rel=mask-icon href=https://brianhengen.us/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://brianhengen.us/posts/fine-tuning-management-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://brianhengen.us/posts/fine-tuning-management-ai/"><meta property="og:site_name" content="Brian Hengen's AI Experiments"><meta property="og:title" content="Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes"><meta property="og:description" content="How I fine-tuned IBM Granite 4.0 H Micro on 5+ years of management notes to create a personalized AI assistant—after three failures, the key was data quality."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-05T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="SLM"><meta property="article:tag" content="Fine-Tuning"><meta property="article:tag" content="Granite"><meta property="article:tag" content="Management AI"><meta property="article:tag" content="Executive Assistant"><meta name=twitter:card content="summary"><meta name=twitter:title content="Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes"><meta name=twitter:description content="How I fine-tuned IBM Granite 4.0 H Micro on 5+ years of management notes to create a personalized AI assistant—after three failures, the key was data quality."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://brianhengen.us/posts/"},{"@type":"ListItem","position":2,"name":"Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes","item":"https://brianhengen.us/posts/fine-tuning-management-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes","name":"Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes","description":"How I fine-tuned IBM Granite 4.0 H Micro on 5+ years of management notes to create a personalized AI assistant—after three failures, the key was data quality.","keywords":["AI","SLM","Fine-tuning","Granite","Management AI","Executive Assistant"],"articleBody":"After successfully fine-tuning ChefBot on cooking recipes, I wondered: Could I fine-tune an AI on my own management experience to create a personalized executive assistant?\nImagine asking your AI: “Summarize last week’s 1:1 with Sarah and suggest coaching points”, and getting a response in your voice, drawing from years of team dynamics and decision patterns. All running locally, with complete privacy and no API costs.\nThis is the story of how I built exactly that. Spoiler: I failed three times before succeeding, and the lesson wasn’t about hyperparameters.\nWhy Build This? As a technology executive, I generate mountains of notes—hundreds of pages across:\nMeetings with team members Leadership team strategy discussions Deal reviews and account planning Performance coaching sessions Risk assessments and escalations General-purpose LLMs like ChatGPT can help with generic management advice, but they don’t know:\nMy specific management philosophy and approach My team’s unique dynamics and challenges My company’s internal processes and terminology The context from years of accumulated decisions The goal: Train a Specialized Language Model (SLM) that acts like my personal management consultant, tuned on the last 3 years of my personal notes.\nBonus benefits:\n✅ Privacy: All data stays on-premises (critical for sensitive data) ✅ No API costs: One-time training vs. ongoing ChatGPT fees ✅ Customization: Full control over behavior and outputs ✅ Learning: Hands-on experience with real-world fine-tuning challenges The Hardware: RTX 5090 Strikes Again My RTX 5090 workstation proved perfect for this project:\n32GB VRAM: Enough for 3B parameter models with QLoRA AMD Ryzen 9 7900X (64GB RAM): Fast data preprocessing Local training: No cloud dependencies or data transfer For 3B parameter models, the 5090 is actually overkill—but that headroom means I can experiment with larger models later.\nChoosing the Right Model After researching options, I chose IBM Granite 4.0 H Micro over the more common Llama 3.2 3B:\nHere’s a quick comparison:\n{ \"type\": \"bar\", \"data\": { \"labels\": [\"Parameters\", \"Context Window\", \"HumanEval\"], \"datasets\": [ { \"label\": \"Granite 4.0 H Micro\", \"data\": [3, 128, 81], \"backgroundColor\": \"#4e79a7\" }, { \"label\": \"Llama 3.2 3B\", \"data\": [3, 8, 75], \"backgroundColor\": \"#f28e2b\" } ] }, \"options\": { \"scales\": { \"y\": { \"beginAtZero\": true } }, \"plugins\": { \"legend\": { \"position\": \"top\" }, \"title\": { \"display\": true, \"text\": \"Granite 4.0 H Micro vs Llama 3.2 3B\" } } } } Feature Granite 4.0 H Micro Llama 3.2 3B Parameters 3B 3B Context Window 128K tokens 4K-8K tokens Architecture Hybrid (4 attention + 36 Mamba2 layers) Standard transformer License Apache 2.0 Llama 3 Community Enterprise Focus ✅ Built for business ❌ General purpose Deployment Optimized for MacBook Pro Standard Why Granite won:\n128K context window is crucial for long management notes and meeting transcripts Hybrid Mamba2 architecture is more efficient than pure transformers for long context Enterprise-focused design aligns with my use case Tool-calling capabilities for future RAG integration Only 3B parameters but matches/beats larger models on benchmarks (MMLU: 67.4, HumanEval: 81) The Data Disaster: Three Failed Attempts Attempt 1: The Overfit Catastrophe Setup: 3 epochs, 25,516 training examples, LoRA rank 64\nResult: COMPLETE FAILURE\nTraining loss: 2.46 → 0.0006 (99.97% reduction!) Eval loss: nan (can’t generalize at all) Duration: ~7 hours The model memorized everything—regurgitating training data verbatim. Worse than the base model.\nAttempt 2: Too Conservative Setup: 1 epoch, reduced learning rate, higher regularization\nResult: Better, but too generic\nNo overfitting (good!) But didn’t learn my management style (bad!) Too cautious—threw out the baby with the bathwater Attempt 3: Still Broken Setup: 2 epochs (compromise between V1 and V2)\nResult: Still overfitting\nTraining loss: 0.02-0.03 (too low) Eval loss: nan again Model output correct JSON but filled with gibberish At this point, I realized: You can’t fix bad data with better hyperparameters.\nThe Real Problem: Garbage Data I examined the training data more closely:\nBad Example 1:\n{ \"question\": \"What is the main insight?\", \"answer\": \"{\\\"summary\\\":\\\"FY21 - Leadership Team.pdf\\\",\\\"bullets\\\":[]}\" } Bad Example 2:\n{ \"question\": \"How can this be implemented?\", \"answer\": \"{\\\"items\\\":[{\\\"text\\\":\\\"Bob got engaged yesterday!\\\"}]}\" } The data was auto-generated by weak AI and never validated:\nEmpty summaries (just filenames) Random text fragments as “action items” Generic template questions with zero variation No actual insight extraction The model learned perfectly—it’s just that it was learning garbage. Pro tip: Always sample and validate your dataset manually before training.\nThe Solution: Regenerate with Quality I went back to the source: 96 PDF files of real management notes:\n201MB of raw notes Extracted into 1,623 text chunks Covering 1:1 meetings, leadership discussions, deal reviews, coaching sessions New strategy: Use my company’s coding assistant to generate quality training examples:\nFeed each chunk to the assistant Ask it to generate 2-3 natural, conversational questions with real insights Extract summaries, action items, risks, and coaching advice Focus on quality over quantity Output: 2,946 high-quality examples (vs 25,516 garbage ones)\nQuality comparison:\nOLD (garbage):\n{ \"question\": \"What is the main insight?\", \"answer\": \"{\\\"summary\\\":\\\"FY21 - Leadership Team.pdf\\\",\\\"bullets\\\":[]}\" } NEW (quality):\n{ \"question\": \"How can a leader help maintain productive tension during organizational change?\", \"answer\": \"A leader must: 1) Create a 'holding environment' where diverse groups can discuss challenges, 2) Sequence and pace the work so people don't feel overwhelmed, 3) Regulate distress and maintain emotional capacity to tolerate uncertainty.\" } The difference is night and day. Pro tip: Use a strong API for data augmentation - it makes a big difference\nAttempt 4: Success at Last The Setup:\nModel: IBM Granite 4.0 H Micro (3B params) Data: 2,651 train / 295 validation (90/10 split) Training: 2 epochs, QLoRA (4-bit quantization), LoRA rank 32 Duration: 1 hour 25 minutes The Results:\n✅ Training loss: 2.403 ✅ Eval loss: 2.299 (healthy gap—no overfitting!) ✅ 664 total steps (vs 5,742 with bloated dataset) ✅ 10x faster than previous attempts ✅ Model actually works! The Proof: Real-World Testing I tested the tuned model against the base Granite model with real management queries:\nEmail Summarization Tuned: 40% more concise, captures key points in executive-friendly format Base: Comprehensive but verbose, too much detail Technical Questions Tuned: 8 actionable bullet points, executive-ready Base: 11+ paragraphs of academic detail Context Awareness Tuned: Recognizes specific details from my training data Base: Generic responses without organizational context Management Advice Tuned: Concise next steps with clear priorities Base: 4-section implementation plan (too detailed for quick decisions) The verdict: The fine-tuned model is significantly more useful for daily management tasks, mirroring my voice and priorities.\nKey Lessons Learned 1. Data Quality Trumps Everything You cannot fix bad data with hyperparameter tuning. The model will learn perfectly—it’s just that it will learn the wrong patterns.\n❌ 25,516 garbage examples = failure ✅ 2,946 quality examples = success Spend time on data quality upfront. It’s worth it to generate quality training data rather than weak models that create garbage.\n2. The Sweet Spot: 2 Epochs with Quality Data 1 epoch: Underfit (too generic) 2 epochs: Just right ✅ 3 epochs: Overfitting risk (even with good data) With quality data, you need fewer epochs to achieve good results.\n3. Smaller, Focused Datasets Beat Large Garbage Datasets 2,651 quality examples »\u003e 25,516 garbage examples Faster training (1.5 hours vs 7+ hours) Better results Less risk of overfitting Quality over quantity is not just a slogan—it’s the difference between success and failure.\n4. Model Architecture Matters Granite 4.0 H Micro’s hybrid Mamba2 architecture offers real advantages:\nMore efficient than standard transformers Better long-context handling (crucial for meeting notes) Optimized for edge deployment (runs great on MacBook Pro) Only 3B params but excellent performance Don’t just default to Llama models—explore alternatives that might fit your use case better.\n5. Watch the Eval Loss Gap Train: 2.40, Eval: 2.30 → Healthy ✅ Train: 0.0006, Eval: nan → Disaster ❌ A healthy gap between training and eval loss indicates proper generalization. If training loss goes near zero while eval loss explodes, you’re overfitting.\nProduction Deployment Model Status: ✅ READY FOR PRODUCTION\nDeployment specs:\nLoRA adapters: Only 6.6MB (easy to distribute) Memory footprint: ~2GB with 4-bit quantization Inference speed: ~7 seconds per response Runs on MacBook Pro (no GPU required for inference) Planned architecture:\nFine-tuned model for management style and decision-making RAG (Retrieval-Augmented Generation) for specific knowledge This keeps the management voice pure while adding factual product info Why RAG instead of more fine-tuning?\nProduct knowledge changes frequently (RAG is easier to update) Prevents diluting the management style learned in V4 Best of both worlds: personal style + current facts ROI:\nCustom AI assistant tuned to 3+ years of management experience Runs locally (privacy guaranteed) No ongoing API costs Ready to deploy on laptop What’s Next? Immediate:\nTest against daily management tasks Gather feedback on gaps in knowledge Build RAG system for ongoing practices Got questions about fine-tuning your own domain-specific AI? Drop a comment or reach out on GitHub!\nThe Big Takeaway After four attempts and multiple failures, here’s what I learned:\nBuilding specialized AI isn’t about having the biggest model or the most complex training setup.\nIt’s about:\nQuality data (spend money/time here) Right-sized model (3B can beat 70B with good data) Clear evaluation metrics (watch that eval loss!) Patience to iterate (fail fast, learn, improve) The RTX 5090 made experimentation easy—I could try different approaches in hours rather than days. But the real breakthrough came from recognizing that garbage data can’t be fixed with engineering tricks.\nIf you’re considering fine-tuning an LLM for your domain, invest in data quality first. Everything else follows from that foundation.\nProject Stats:\nStarted: October 3, 2025 Production Ready: October 4, 2025 (2 days!) Training time: 1 hour 25 minutes (V4) Data: 2,946 examples from 5+ years of notes Cost: \u003c$20 total Model: IBM Granite 4.0 H Micro (3B params) Want to try this yourself? The key components:\nRTX GPU (3090/4090/5090) or cloud compute Quality source data (your domain expertise) QLoRA fine-tuning (Hugging Face PEFT) Patience to iterate on data quality ","wordCount":"1634","inLanguage":"en","datePublished":"2025-10-05T00:00:00Z","dateModified":"2025-10-05T00:00:00Z","author":{"@type":"Person","name":"Brian Hengen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://brianhengen.us/posts/fine-tuning-management-ai/"},"publisher":{"@type":"Organization","name":"Brian Hengen's AI Experiments","logo":{"@type":"ImageObject","url":"https://brianhengen.us/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://brianhengen.us/ accesskey=h title="Brian Hengen's AI Experiments (Alt + H)">Brian Hengen's AI Experiments</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://brianhengen.us/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://brianhengen.us/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://brianhengen.us/>Home</a>&nbsp;»&nbsp;<a href=https://brianhengen.us/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes</h1><div class=post-description>How I fine-tuned IBM Granite 4.0 H Micro on 5+ years of management notes to create a personalized AI assistant—after three failures, the key was data quality.</div><div class=post-meta><span title='2025-10-05 00:00:00 +0000 UTC'>October 5, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Brian Hengen</div></header><div class=post-content><p>After successfully fine-tuning ChefBot on cooking recipes, I wondered: <strong>Could I fine-tune an AI on my own management experience to create a personalized executive assistant?</strong></p><p>Imagine asking your AI: &ldquo;Summarize last week&rsquo;s 1:1 with Sarah and suggest coaching points&rdquo;, and getting a response in your voice, drawing from years of team dynamics and decision patterns. All running locally, with complete privacy and no API costs.</p><p>This is the story of how I built exactly that. Spoiler: I failed three times before succeeding, and the lesson wasn&rsquo;t about hyperparameters.</p><h2 id=why-build-this>Why Build This?<a hidden class=anchor aria-hidden=true href=#why-build-this>#</a></h2><p>As a technology executive, I generate mountains of notes—hundreds of pages across:</p><ul><li>Meetings with team members</li><li>Leadership team strategy discussions</li><li>Deal reviews and account planning</li><li>Performance coaching sessions</li><li>Risk assessments and escalations</li></ul><p>General-purpose LLMs like ChatGPT can help with generic management advice, but they don&rsquo;t know:</p><ul><li>My specific management philosophy and approach</li><li>My team&rsquo;s unique dynamics and challenges</li><li>My company&rsquo;s internal processes and terminology</li><li>The context from years of accumulated decisions</li></ul><p><strong>The goal:</strong> Train a Specialized Language Model (SLM) that acts like my personal management consultant, tuned on the last 3 years of my personal notes.</p><p><strong>Bonus benefits:</strong></p><ul><li>✅ Privacy: All data stays on-premises (critical for sensitive data)</li><li>✅ No API costs: One-time training vs. ongoing ChatGPT fees</li><li>✅ Customization: Full control over behavior and outputs</li><li>✅ Learning: Hands-on experience with real-world fine-tuning challenges</li></ul><h2 id=the-hardware-rtx-5090-strikes-again>The Hardware: RTX 5090 Strikes Again<a hidden class=anchor aria-hidden=true href=#the-hardware-rtx-5090-strikes-again>#</a></h2><p>My RTX 5090 workstation proved perfect for this project:</p><ul><li><strong>32GB VRAM</strong>: Enough for 3B parameter models with QLoRA</li><li><strong>AMD Ryzen 9 7900X (64GB RAM)</strong>: Fast data preprocessing</li><li><strong>Local training</strong>: No cloud dependencies or data transfer</li></ul><p>For 3B parameter models, the 5090 is actually overkill—but that headroom means I can experiment with larger models later.</p><h2 id=choosing-the-right-model>Choosing the Right Model<a hidden class=anchor aria-hidden=true href=#choosing-the-right-model>#</a></h2><p>After researching options, I chose <strong><a href=https://huggingface.co/ibm-granite/granite-4.0-h-micro>IBM Granite 4.0 H Micro</a></strong> over the more common Llama 3.2 3B:</p><p>Here&rsquo;s a quick comparison:</p><pre tabindex=0><code class=language-chartjs data-lang=chartjs>{
  &#34;type&#34;: &#34;bar&#34;,
  &#34;data&#34;: {
    &#34;labels&#34;: [&#34;Parameters&#34;, &#34;Context Window&#34;, &#34;HumanEval&#34;],
    &#34;datasets&#34;: [
      {
        &#34;label&#34;: &#34;Granite 4.0 H Micro&#34;,
        &#34;data&#34;: [3, 128, 81],
        &#34;backgroundColor&#34;: &#34;#4e79a7&#34;
      },
      {
        &#34;label&#34;: &#34;Llama 3.2 3B&#34;,
        &#34;data&#34;: [3, 8, 75],
        &#34;backgroundColor&#34;: &#34;#f28e2b&#34;
      }
    ]
  },
  &#34;options&#34;: {
    &#34;scales&#34;: {
      &#34;y&#34;: {
        &#34;beginAtZero&#34;: true
      }
    },
    &#34;plugins&#34;: {
      &#34;legend&#34;: {
        &#34;position&#34;: &#34;top&#34;
      },
      &#34;title&#34;: {
        &#34;display&#34;: true,
        &#34;text&#34;: &#34;Granite 4.0 H Micro vs Llama 3.2 3B&#34;
      }
    }
  }
}
</code></pre><table><thead><tr><th>Feature</th><th>Granite 4.0 H Micro</th><th>Llama 3.2 3B</th></tr></thead><tbody><tr><td>Parameters</td><td>3B</td><td>3B</td></tr><tr><td>Context Window</td><td><strong>128K tokens</strong></td><td>4K-8K tokens</td></tr><tr><td>Architecture</td><td>Hybrid (4 attention + 36 Mamba2 layers)</td><td>Standard transformer</td></tr><tr><td>License</td><td>Apache 2.0</td><td>Llama 3 Community</td></tr><tr><td>Enterprise Focus</td><td>✅ Built for business</td><td>❌ General purpose</td></tr><tr><td>Deployment</td><td>Optimized for MacBook Pro</td><td>Standard</td></tr></tbody></table><p><strong>Why Granite won:</strong></p><ul><li><strong>128K context window</strong> is crucial for long management notes and meeting transcripts</li><li><strong>Hybrid Mamba2 architecture</strong> is more efficient than pure transformers for long context</li><li><strong>Enterprise-focused</strong> design aligns with my use case</li><li><strong>Tool-calling capabilities</strong> for future RAG integration</li><li>Only 3B parameters but matches/beats larger models on benchmarks (MMLU: 67.4, HumanEval: 81)</li></ul><h2 id=the-data-disaster-three-failed-attempts>The Data Disaster: Three Failed Attempts<a hidden class=anchor aria-hidden=true href=#the-data-disaster-three-failed-attempts>#</a></h2><h3 id=attempt-1-the-overfit-catastrophe>Attempt 1: The Overfit Catastrophe<a hidden class=anchor aria-hidden=true href=#attempt-1-the-overfit-catastrophe>#</a></h3><p><strong>Setup:</strong> 3 epochs, 25,516 training examples, LoRA rank 64</p><p><strong>Result:</strong> COMPLETE FAILURE</p><ul><li>Training loss: 2.46 → <strong>0.0006</strong> (99.97% reduction!)</li><li>Eval loss: <strong>nan</strong> (can&rsquo;t generalize at all)</li><li>Duration: ~7 hours</li></ul><p>The model memorized everything—regurgitating training data verbatim. Worse than the base model.</p><h3 id=attempt-2-too-conservative>Attempt 2: Too Conservative<a hidden class=anchor aria-hidden=true href=#attempt-2-too-conservative>#</a></h3><p><strong>Setup:</strong> 1 epoch, reduced learning rate, higher regularization</p><p><strong>Result:</strong> Better, but too generic</p><ul><li>No overfitting (good!)</li><li>But didn&rsquo;t learn my management style (bad!)</li><li>Too cautious—threw out the baby with the bathwater</li></ul><h3 id=attempt-3-still-broken>Attempt 3: Still Broken<a hidden class=anchor aria-hidden=true href=#attempt-3-still-broken>#</a></h3><p><strong>Setup:</strong> 2 epochs (compromise between V1 and V2)</p><p><strong>Result:</strong> Still overfitting</p><ul><li>Training loss: 0.02-0.03 (too low)</li><li>Eval loss: nan again</li><li>Model output correct JSON but filled with gibberish</li></ul><p>At this point, I realized: <strong>You can&rsquo;t fix bad data with better hyperparameters.</strong></p><h2 id=the-real-problem-garbage-data>The Real Problem: Garbage Data<a hidden class=anchor aria-hidden=true href=#the-real-problem-garbage-data>#</a></h2><p>I examined the training data more closely:</p><p><strong>Bad Example 1:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;What is the main insight?&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;{\&#34;summary\&#34;:\&#34;FY21 - Leadership Team.pdf\&#34;,\&#34;bullets\&#34;:[]}&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>Bad Example 2:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;How can this be implemented?&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;{\&#34;items\&#34;:[{\&#34;text\&#34;:\&#34;Bob got engaged yesterday!\&#34;}]}&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The data was auto-generated by weak AI and never validated:</p><ul><li>Empty summaries (just filenames)</li><li>Random text fragments as &ldquo;action items&rdquo;</li><li>Generic template questions with zero variation</li><li>No actual insight extraction</li></ul><p><strong>The model learned perfectly—it&rsquo;s just that it was learning garbage.</strong> Pro tip: Always sample and validate your dataset manually before training.</p><h2 id=the-solution-regenerate-with-quality>The Solution: Regenerate with Quality<a hidden class=anchor aria-hidden=true href=#the-solution-regenerate-with-quality>#</a></h2><p>I went back to the source: <strong>96 PDF files of real management notes</strong>:</p><ul><li>201MB of raw notes</li><li>Extracted into 1,623 text chunks</li><li>Covering 1:1 meetings, leadership discussions, deal reviews, coaching sessions</li></ul><p><strong>New strategy:</strong> Use my company&rsquo;s coding assistant to generate quality training examples:</p><ol><li>Feed each chunk to the assistant</li><li>Ask it to generate 2-3 natural, conversational questions with real insights</li><li>Extract summaries, action items, risks, and coaching advice</li><li>Focus on quality over quantity</li></ol><p><strong>Output:</strong> 2,946 high-quality examples (vs 25,516 garbage ones)</p><p><strong>Quality comparison:</strong></p><p><strong>OLD (garbage):</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;What is the main insight?&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;{\&#34;summary\&#34;:\&#34;FY21 - Leadership Team.pdf\&#34;,\&#34;bullets\&#34;:[]}&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>NEW (quality):</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;How can a leader help maintain productive tension during organizational change?&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;A leader must: 1) Create a &#39;holding environment&#39; where diverse groups can discuss challenges, 2) Sequence and pace the work so people don&#39;t feel overwhelmed, 3) Regulate distress and maintain emotional capacity to tolerate uncertainty.&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The difference is night and day. Pro tip: Use a strong API for data augmentation - it makes a big difference</p><h2 id=attempt-4-success-at-last>Attempt 4: Success at Last<a hidden class=anchor aria-hidden=true href=#attempt-4-success-at-last>#</a></h2><p><strong>The Setup:</strong></p><ul><li><strong>Model:</strong> IBM Granite 4.0 H Micro (3B params)</li><li><strong>Data:</strong> 2,651 train / 295 validation (90/10 split)</li><li><strong>Training:</strong> 2 epochs, QLoRA (4-bit quantization), LoRA rank 32</li><li><strong>Duration:</strong> 1 hour 25 minutes</li></ul><p><strong>The Results:</strong></p><ul><li>✅ Training loss: <strong>2.403</strong></li><li>✅ Eval loss: <strong>2.299</strong> (healthy gap—no overfitting!)</li><li>✅ 664 total steps (vs 5,742 with bloated dataset)</li><li>✅ <strong>10x faster</strong> than previous attempts</li><li>✅ Model actually works!</li></ul><h2 id=the-proof-real-world-testing>The Proof: Real-World Testing<a hidden class=anchor aria-hidden=true href=#the-proof-real-world-testing>#</a></h2><p>I tested the tuned model against the base Granite model with real management queries:</p><h3 id=email-summarization>Email Summarization<a hidden class=anchor aria-hidden=true href=#email-summarization>#</a></h3><ul><li><strong>Tuned:</strong> 40% more concise, captures key points in executive-friendly format</li><li><strong>Base:</strong> Comprehensive but verbose, too much detail</li></ul><h3 id=technical-questions>Technical Questions<a hidden class=anchor aria-hidden=true href=#technical-questions>#</a></h3><ul><li><strong>Tuned:</strong> 8 actionable bullet points, executive-ready</li><li><strong>Base:</strong> 11+ paragraphs of academic detail</li></ul><h3 id=context-awareness>Context Awareness<a hidden class=anchor aria-hidden=true href=#context-awareness>#</a></h3><ul><li><strong>Tuned:</strong> Recognizes specific details from my training data</li><li><strong>Base:</strong> Generic responses without organizational context</li></ul><h3 id=management-advice>Management Advice<a hidden class=anchor aria-hidden=true href=#management-advice>#</a></h3><ul><li><strong>Tuned:</strong> Concise next steps with clear priorities</li><li><strong>Base:</strong> 4-section implementation plan (too detailed for quick decisions)</li></ul><p><strong>The verdict:</strong> The fine-tuned model is <strong>significantly more useful</strong> for daily management tasks, mirroring my voice and priorities.</p><h2 id=key-lessons-learned>Key Lessons Learned<a hidden class=anchor aria-hidden=true href=#key-lessons-learned>#</a></h2><h3 id=1-data-quality-trumps-everything>1. Data Quality Trumps Everything<a hidden class=anchor aria-hidden=true href=#1-data-quality-trumps-everything>#</a></h3><p><strong>You cannot fix bad data with hyperparameter tuning.</strong> The model will learn perfectly—it&rsquo;s just that it will learn the wrong patterns.</p><ul><li>❌ 25,516 garbage examples = failure</li><li>✅ 2,946 quality examples = success</li></ul><p>Spend time on data quality upfront. It&rsquo;s worth it to generate quality training data rather than weak models that create garbage.</p><h3 id=2-the-sweet-spot-2-epochs-with-quality-data>2. The Sweet Spot: 2 Epochs with Quality Data<a hidden class=anchor aria-hidden=true href=#2-the-sweet-spot-2-epochs-with-quality-data>#</a></h3><ul><li><strong>1 epoch:</strong> Underfit (too generic)</li><li><strong>2 epochs:</strong> Just right ✅</li><li><strong>3 epochs:</strong> Overfitting risk (even with good data)</li></ul><p>With quality data, you need fewer epochs to achieve good results.</p><h3 id=3-smaller-focused-datasets-beat-large-garbage-datasets>3. Smaller, Focused Datasets Beat Large Garbage Datasets<a hidden class=anchor aria-hidden=true href=#3-smaller-focused-datasets-beat-large-garbage-datasets>#</a></h3><ul><li>2,651 quality examples &#187;> 25,516 garbage examples</li><li>Faster training (1.5 hours vs 7+ hours)</li><li>Better results</li><li>Less risk of overfitting</li></ul><p><strong>Quality over quantity</strong> is not just a slogan—it&rsquo;s the difference between success and failure.</p><h3 id=4-model-architecture-matters>4. Model Architecture Matters<a hidden class=anchor aria-hidden=true href=#4-model-architecture-matters>#</a></h3><p>Granite 4.0 H Micro&rsquo;s hybrid Mamba2 architecture offers real advantages:</p><ul><li>More efficient than standard transformers</li><li>Better long-context handling (crucial for meeting notes)</li><li>Optimized for edge deployment (runs great on MacBook Pro)</li><li>Only 3B params but excellent performance</li></ul><p>Don&rsquo;t just default to Llama models—explore alternatives that might fit your use case better.</p><h3 id=5-watch-the-eval-loss-gap>5. Watch the Eval Loss Gap<a hidden class=anchor aria-hidden=true href=#5-watch-the-eval-loss-gap>#</a></h3><ul><li>Train: 2.40, Eval: 2.30 → <strong>Healthy</strong> ✅</li><li>Train: 0.0006, Eval: nan → <strong>Disaster</strong> ❌</li></ul><p>A healthy gap between training and eval loss indicates proper generalization. If training loss goes near zero while eval loss explodes, you&rsquo;re overfitting.</p><h2 id=production-deployment>Production Deployment<a hidden class=anchor aria-hidden=true href=#production-deployment>#</a></h2><p><strong>Model Status:</strong> ✅ READY FOR PRODUCTION</p><p><strong>Deployment specs:</strong></p><ul><li>LoRA adapters: Only 6.6MB (easy to distribute)</li><li>Memory footprint: ~2GB with 4-bit quantization</li><li>Inference speed: ~7 seconds per response</li><li>Runs on MacBook Pro (no GPU required for inference)</li></ul><p><strong>Planned architecture:</strong></p><ol><li>Fine-tuned model for management style and decision-making</li><li>RAG (Retrieval-Augmented Generation) for specific knowledge</li><li>This keeps the management voice pure while adding factual product info</li></ol><p>Why RAG instead of more fine-tuning?</p><ul><li>Product knowledge changes frequently (RAG is easier to update)</li><li>Prevents diluting the management style learned in V4</li><li>Best of both worlds: personal style + current facts</li></ul><p><strong>ROI:</strong></p><ul><li>Custom AI assistant tuned to 3+ years of management experience</li><li>Runs locally (privacy guaranteed)</li><li>No ongoing API costs</li><li>Ready to deploy on laptop</li></ul><h2 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p><strong>Immediate:</strong></p><ul><li>Test against daily management tasks</li><li>Gather feedback on gaps in knowledge</li><li>Build RAG system for ongoing practices</li></ul><p>Got questions about fine-tuning your own domain-specific AI? Drop a comment or reach out on GitHub!</p><h2 id=the-big-takeaway>The Big Takeaway<a hidden class=anchor aria-hidden=true href=#the-big-takeaway>#</a></h2><p>After four attempts and multiple failures, here&rsquo;s what I learned:</p><p><strong>Building specialized AI isn&rsquo;t about having the biggest model or the most complex training setup.</strong></p><p>It&rsquo;s about:</p><ol><li><strong>Quality data</strong> (spend money/time here)</li><li><strong>Right-sized model</strong> (3B can beat 70B with good data)</li><li><strong>Clear evaluation metrics</strong> (watch that eval loss!)</li><li><strong>Patience to iterate</strong> (fail fast, learn, improve)</li></ol><p>The RTX 5090 made experimentation easy—I could try different approaches in hours rather than days. But the real breakthrough came from recognizing that <strong>garbage data can&rsquo;t be fixed with engineering tricks</strong>.</p><p>If you&rsquo;re considering fine-tuning an LLM for your domain, invest in data quality first. Everything else follows from that foundation.</p><hr><p><strong>Project Stats:</strong></p><ul><li><strong>Started:</strong> October 3, 2025</li><li><strong>Production Ready:</strong> October 4, 2025 (2 days!)</li><li><strong>Training time:</strong> 1 hour 25 minutes (V4)</li><li><strong>Data:</strong> 2,946 examples from 5+ years of notes</li><li><strong>Cost:</strong> &lt;$20 total</li><li><strong>Model:</strong> IBM Granite 4.0 H Micro (3B params)</li></ul><p>Want to try this yourself? The key components:</p><ul><li>RTX GPU (3090/4090/5090) or cloud compute</li><li>Quality source data (your domain expertise)</li><li>QLoRA fine-tuning (Hugging Face PEFT)</li><li>Patience to iterate on data quality</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://brianhengen.us/tags/ai/>AI</a></li><li><a href=https://brianhengen.us/tags/slm/>SLM</a></li><li><a href=https://brianhengen.us/tags/fine-tuning/>Fine-Tuning</a></li><li><a href=https://brianhengen.us/tags/granite/>Granite</a></li><li><a href=https://brianhengen.us/tags/management-ai/>Management AI</a></li><li><a href=https://brianhengen.us/tags/executive-assistant/>Executive Assistant</a></li></ul><nav class=paginav><a class=next href=https://brianhengen.us/posts/fine-tuning-qwen-models/><span class=title>Next »</span><br><span>Fine-Tuning Qwen Models: From Theory to Practice</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on x" href="https://x.com/intent/tweet/?text=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f&amp;hashtags=AI%2cSLM%2cFine-tuning%2cGranite%2cManagementAI%2cExecutiveAssistant"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f&amp;title=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes&amp;summary=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes&amp;source=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f&title=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on whatsapp" href="https://api.whatsapp.com/send?text=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes%20-%20https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on telegram" href="https://telegram.me/share/url?text=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes on ycombinator" href="https://news.ycombinator.com/submitlink?t=Fine-Tuning%20a%20Personal%20Executive%20Assistant%3a%20Lessons%20from%20My%20Management%20Notes&u=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-management-ai%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://brianhengen.us/>Brian Hengen's AI Experiments</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div id=mc_embed_shell style="max-width:600px;margin:2rem auto;padding:1.5rem;text-align:center;border-top:1px solid var(--border)"><div id=mc_embed_signup><form action="https://brianhengen.us18.list-manage.com/subscribe/post?u=6873340033b37c32cd7ea9c63&amp;id=ec4a1dcc46&amp;f_id=0077b2e6f0" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class=validate target=_blank><div id=mc_embed_signup_scroll><h3 style=margin-bottom:.5rem>Subscribe to New Posts</h3><p style=margin-bottom:1rem;font-size:.9rem>Get notified when I publish new articles about AI/ML training and workstation builds.</p><div class=mc-field-group style=margin-bottom:1rem><input type=email name=EMAIL class="required email" id=mce-EMAIL required placeholder="Your email address" style="padding:.5rem;width:100%;max-width:300px;border:1px solid var(--border);border-radius:4px;background:var(--entry);color:var(--primary)"></div><div id=mce-responses class=clear><div class=response id=mce-error-response style=display:none></div><div class=response id=mce-success-response style=display:none></div></div><div aria-hidden=true style=position:absolute;left:-5e3px><input type=text name=b_6873340033b37c32cd7ea9c63_ec4a1dcc46 tabindex=-1></div><div class=clear><input type=submit name=subscribe id=mc-embedded-subscribe class=button value=Subscribe style="padding:.5rem 2rem;background:var(--theme);color:var(--entry);border:none;border-radius:4px;cursor:pointer;font-weight:600"></div></div></form></div></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>