<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine-Tuning Qwen Models: From Theory to Practice | Brian Hengen's AI Experiments</title><meta name=keywords content="AI,SLM,Fine-tuning,Qwen,ChefBot,RecipeNLG,Machine Learning"><meta name=description content="Fine-tuning Qwen 32B and 14B models to create ChefBot‚Äîa cooking AI that outperforms general-purpose LLMs in the kitchen."><meta name=author content="Brian Hengen"><link rel=canonical href=https://brianhengen.us/posts/fine-tuning-qwen-models/><link crossorigin=anonymous href=/assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://brianhengen.us/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://brianhengen.us/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://brianhengen.us/favicon-32x32.png><link rel=apple-touch-icon href=https://brianhengen.us/apple-touch-icon.png><link rel=mask-icon href=https://brianhengen.us/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://brianhengen.us/posts/fine-tuning-qwen-models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://brianhengen.us/posts/fine-tuning-qwen-models/"><meta property="og:site_name" content="Brian Hengen's AI Experiments"><meta property="og:title" content="Fine-Tuning Qwen Models: From Theory to Practice"><meta property="og:description" content="Fine-tuning Qwen 32B and 14B models to create ChefBot‚Äîa cooking AI that outperforms general-purpose LLMs in the kitchen."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-29T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="SLM"><meta property="article:tag" content="Fine-Tuning"><meta property="article:tag" content="Qwen"><meta property="article:tag" content="ChefBot"><meta property="article:tag" content="RecipeNLG"><meta name=twitter:card content="summary"><meta name=twitter:title content="Fine-Tuning Qwen Models: From Theory to Practice"><meta name=twitter:description content="Fine-tuning Qwen 32B and 14B models to create ChefBot‚Äîa cooking AI that outperforms general-purpose LLMs in the kitchen."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://brianhengen.us/posts/"},{"@type":"ListItem","position":2,"name":"Fine-Tuning Qwen Models: From Theory to Practice","item":"https://brianhengen.us/posts/fine-tuning-qwen-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine-Tuning Qwen Models: From Theory to Practice","name":"Fine-Tuning Qwen Models: From Theory to Practice","description":"Fine-tuning Qwen 32B and 14B models to create ChefBot‚Äîa cooking AI that outperforms general-purpose LLMs in the kitchen.","keywords":["AI","SLM","Fine-tuning","Qwen","ChefBot","RecipeNLG","Machine Learning"],"articleBody":"Imagine an AI that coordinates your entire cooking process‚Äîfaster, smarter, and without ChatGPT‚Äôs API costs. With my RTX 5090 workstation humming, I‚Äôm answering: Can a specialized language model outcook ChatGPT in the kitchen?\nOver the past few days, I‚Äôve been fine-tuning Qwen‚Äôs 32B and 14B parameter models to create ChefBot, an experimental Specialized Language Model (SLM). Think ChatGPT for recipes, but tuned for cooking data and running without per-token API costs.\nWhy ChefBot? Cooking is collaborative, messy, and highly domain-specific. Picture cooking a three-course meal for friends, juggling prep times, oven space, and tasks for your novice sous-chef. General-purpose models like ChatGPT can suggest recipes, but they often miss the nuance of kitchen chaos. That‚Äôs why I‚Äôm building ChefBot to:\nMaster ingredient prep timing (e.g., chop onions while the oven preheats) Sequence cooking methods (e.g., sear steak before roasting) Coordinate equipment (e.g., use one pan for multiple dishes) Allocate tasks by skill (e.g., simple chopping for beginners) A specialized model should handle this better, faster, and cheaper than a general-purpose LLM.\nCan the RTX 5090 Really Handle It? The RTX 5090‚Äôs 32GB of VRAM is a game-changer for training specialized models. Here‚Äôs how it‚Äôs performing:\nQwen-32B ‚úÖ Completed 1000+ step training cycle üìâ Training loss: 1.18 ‚Üí 0.68 (42% reduction) üß† Memory use: Fits comfortably in 32GB ‚è±Ô∏è Time: Several hours per cycle Qwen-14B ‚è≥ Currently at step 5,500 / 150,000 üìâ Eval loss: 1.0 ‚Üí 0.018 ‚ö° Speed: ~2‚Äì3 min per 50 steps (post-optimization) üß† Memory use: ~21GB That 0.018 eval loss? That‚Äôs like a Michelin star for a model.\nModel VRAM Use Training Speed Eval Loss Practicality Qwen-32B 32GB Hours / 1k steps 0.68 Powerful but resource-heavy Qwen-14B 21GB 2‚Äì3 min / 50 steps 0.018 Ideal for speed and deployment The 32B model is impressive, but the 14B model is proving more practical: faster to train, easier to host, and surprisingly close in quality.\nTraining Lessons üîß Configuration Matters Early runs were bottlenecked by CPU preprocessing‚Äîpainfully slow! The fix: crank up parallelism to leverage multi-core CPUs.\n# Before (single-threaded) num_proc: 4 dataloader_num_workers: 0 # After (multi-core optimization) num_proc: 10 dataloader_num_workers: 8 Result: CPU utilization matched the hardware, boosting throughput by ~30%. Pro tip: Check your CPU core count (e.g., lscpu on Linux) and set num_proc to match or slightly exceed it for max efficiency.\nüíæ Checkpointing is Survival Weekend-long training needs resilience:\nsave_steps: 50 # checkpoint every ~2‚Äì5 min save_total_limit: 3 # keep only the last 3 resume_from_checkpoint: true # auto-resume after interruption This lets me pause/restart training without losing progress.\nReal-World Training Data I‚Äôm training on the RecipeNLG dataset (~2.2M recipes from AllRecipes, Food.com, and more), packed with detailed cooking instructions, ingredient lists, and techniques. ChefBot is learning:\nIngredient preparation timing Cooking method sequences Equipment coordination Skill-based task allocation The eval loss drop shows it‚Äôs absorbing real cooking knowledge.\nWhat‚Äôs Next The 14B model is grinding through a 3-epoch (150k step) cycle. Once complete, I‚Äôll:\nBenchmark against GPT-4 in a head-to-head recipe showdown Test it in my kitchen with my wife, cooking a full meal under pressure Optimize for deployment using quantization for lighter, faster hosting Got a favorite recipe for ChefBot to tackle? Drop it in the comments or tweet me at @bhengen!\nThe Business Case Specialized models like ChefBot deliver serious ROI over giant LLMs:\nüí∞ No API costs‚Äîjust ~$50/month hosting vs. $100s in API fees üç≥ Superior cooking knowledge‚Äîtuned on 2.2M recipes üîí Enhanced privacy‚Äîrecipes stay local, no cloud leaks üõ†Ô∏è Full customization‚ÄîI control updates, no waiting for OpenAI This project proves smaller, specialized models can beat giants when context matters.\nStay Tuned Training continues (step 17,500+ with strong convergence). By mid-October, my wife and I will put ChefBot to the test in a real dinner rush. Follow along to see if it outcooks GPT-4!\n","wordCount":"631","inLanguage":"en","datePublished":"2025-09-29T00:00:00Z","dateModified":"2025-09-29T00:00:00Z","author":{"@type":"Person","name":"Brian Hengen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://brianhengen.us/posts/fine-tuning-qwen-models/"},"publisher":{"@type":"Organization","name":"Brian Hengen's AI Experiments","logo":{"@type":"ImageObject","url":"https://brianhengen.us/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://brianhengen.us/ accesskey=h title="Brian Hengen's AI Experiments (Alt + H)">Brian Hengen's AI Experiments</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://brianhengen.us/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://brianhengen.us/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://brianhengen.us/>Home</a>&nbsp;¬ª&nbsp;<a href=https://brianhengen.us/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Fine-Tuning Qwen Models: From Theory to Practice</h1><div class=post-description>Fine-tuning Qwen 32B and 14B models to create ChefBot‚Äîa cooking AI that outperforms general-purpose LLMs in the kitchen.</div><div class=post-meta><span title='2025-09-29 00:00:00 +0000 UTC'>September 29, 2025</span>&nbsp;¬∑&nbsp;3 min&nbsp;¬∑&nbsp;Brian Hengen</div></header><div class=post-content><p>Imagine an AI that coordinates your entire cooking process‚Äîfaster, smarter, and without ChatGPT‚Äôs API costs. With my RTX 5090 workstation humming, I‚Äôm answering: <strong>Can a specialized language model outcook ChatGPT in the kitchen?</strong></p><p>Over the past few days, I‚Äôve been fine-tuning Qwen‚Äôs <strong>32B</strong> and <strong>14B</strong> parameter models to create <strong>ChefBot</strong>, an experimental Specialized Language Model (SLM). Think ChatGPT for recipes, but tuned for cooking data and running without per-token API costs.</p><h2 id=why-chefbot>Why ChefBot?<a hidden class=anchor aria-hidden=true href=#why-chefbot>#</a></h2><p>Cooking is collaborative, messy, and highly domain-specific. Picture cooking a three-course meal for friends, juggling prep times, oven space, and tasks for your novice sous-chef. General-purpose models like ChatGPT can suggest recipes, but they often miss the nuance of kitchen chaos. That‚Äôs why I‚Äôm building <strong>ChefBot</strong> to:</p><ul><li>Master ingredient prep timing (e.g., chop onions while the oven preheats)</li><li>Sequence cooking methods (e.g., sear steak before roasting)</li><li>Coordinate equipment (e.g., use one pan for multiple dishes)</li><li>Allocate tasks by skill (e.g., simple chopping for beginners)</li></ul><p>A specialized model should handle this <strong>better, faster, and cheaper</strong> than a general-purpose LLM.</p><h2 id=can-the-rtx-5090-really-handle-it>Can the RTX 5090 Really Handle It?<a hidden class=anchor aria-hidden=true href=#can-the-rtx-5090-really-handle-it>#</a></h2><p>The RTX 5090‚Äôs <strong>32GB of VRAM</strong> is a game-changer for training specialized models. Here‚Äôs how it‚Äôs performing:</p><h3 id=qwen-32b>Qwen-32B<a hidden class=anchor aria-hidden=true href=#qwen-32b>#</a></h3><ul><li>‚úÖ Completed 1000+ step training cycle</li><li>üìâ Training loss: 1.18 ‚Üí 0.68 (42% reduction)</li><li>üß† Memory use: Fits comfortably in 32GB</li><li>‚è±Ô∏è Time: Several hours per cycle</li></ul><h3 id=qwen-14b>Qwen-14B<a hidden class=anchor aria-hidden=true href=#qwen-14b>#</a></h3><ul><li>‚è≥ Currently at step 5,500 / 150,000</li><li>üìâ Eval loss: 1.0 ‚Üí 0.018</li><li>‚ö° Speed: ~2‚Äì3 min per 50 steps (post-optimization)</li><li>üß† Memory use: ~21GB</li></ul><blockquote><p><em>That 0.018 eval loss? That&rsquo;s like a Michelin star for a model.</em></p></blockquote><table><thead><tr><th>Model</th><th>VRAM Use</th><th>Training Speed</th><th>Eval Loss</th><th>Practicality</th></tr></thead><tbody><tr><td>Qwen-32B</td><td>32GB</td><td>Hours / 1k steps</td><td>0.68</td><td>Powerful but resource-heavy</td></tr><tr><td>Qwen-14B</td><td>21GB</td><td>2‚Äì3 min / 50 steps</td><td>0.018</td><td>Ideal for speed and deployment</td></tr></tbody></table><p>The 32B model is impressive, but the 14B model is proving more practical: faster to train, easier to host, and surprisingly close in quality.</p><h2 id=training-lessons>Training Lessons<a hidden class=anchor aria-hidden=true href=#training-lessons>#</a></h2><h3 id=-configuration-matters>üîß Configuration Matters<a hidden class=anchor aria-hidden=true href=#-configuration-matters>#</a></h3><p>Early runs were bottlenecked by CPU preprocessing‚Äîpainfully slow! The fix: crank up parallelism to leverage multi-core CPUs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># Before (single-threaded)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>num_proc</span>: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span><span style=color:#f92672>dataloader_num_workers</span>: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># After (multi-core optimization)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>num_proc</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span><span style=color:#f92672>dataloader_num_workers</span>: <span style=color:#ae81ff>8</span>
</span></span></code></pre></div><p>Result: CPU utilization matched the hardware, boosting throughput by ~30%. <strong>Pro tip:</strong> Check your CPU core count (e.g., <code>lscpu</code> on Linux) and set <code>num_proc</code> to match or slightly exceed it for max efficiency.</p><h3 id=-checkpointing-is-survival>üíæ Checkpointing is Survival<a hidden class=anchor aria-hidden=true href=#-checkpointing-is-survival>#</a></h3><p>Weekend-long training needs resilience:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>save_steps</span>: <span style=color:#ae81ff>50</span>                <span style=color:#75715e># checkpoint every ~2‚Äì5 min</span>
</span></span><span style=display:flex><span><span style=color:#f92672>save_total_limit</span>: <span style=color:#ae81ff>3</span>           <span style=color:#75715e># keep only the last 3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>resume_from_checkpoint</span>: <span style=color:#66d9ef>true</span>  <span style=color:#75715e># auto-resume after interruption</span>
</span></span></code></pre></div><p>This lets me pause/restart training without losing progress.</p><h2 id=real-world-training-data>Real-World Training Data<a hidden class=anchor aria-hidden=true href=#real-world-training-data>#</a></h2><p>I‚Äôm training on the <a href=https://arxiv.org/abs/2004.10409>RecipeNLG dataset</a> (~2.2M recipes from AllRecipes, Food.com, and more), packed with detailed cooking instructions, ingredient lists, and techniques. ChefBot is learning:</p><ul><li>Ingredient preparation timing</li><li>Cooking method sequences</li><li>Equipment coordination</li><li>Skill-based task allocation</li></ul><p>The eval loss drop shows it‚Äôs absorbing real cooking knowledge.</p><h2 id=whats-next>What‚Äôs Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>The 14B model is grinding through a 3-epoch (150k step) cycle. Once complete, I‚Äôll:</p><ol><li><strong>Benchmark against GPT-4</strong> in a head-to-head recipe showdown</li><li><strong>Test it in my kitchen</strong> with my wife, cooking a full meal under pressure</li><li><strong>Optimize for deployment</strong> using quantization for lighter, faster hosting</li></ol><p>Got a favorite recipe for ChefBot to tackle? Drop it in the comments or tweet me at @bhengen!</p><h2 id=the-business-case>The Business Case<a hidden class=anchor aria-hidden=true href=#the-business-case>#</a></h2><p>Specialized models like ChefBot deliver serious ROI over giant LLMs:</p><ul><li>üí∞ <strong>No API costs</strong>‚Äîjust ~$50/month hosting vs. $100s in API fees</li><li>üç≥ <strong>Superior cooking knowledge</strong>‚Äîtuned on 2.2M recipes</li><li>üîí <strong>Enhanced privacy</strong>‚Äîrecipes stay local, no cloud leaks</li><li>üõ†Ô∏è <strong>Full customization</strong>‚ÄîI control updates, no waiting for OpenAI</li></ul><p>This project proves smaller, specialized models can beat giants when context matters.</p><h2 id=stay-tuned>Stay Tuned<a hidden class=anchor aria-hidden=true href=#stay-tuned>#</a></h2><p>Training continues (step 17,500+ with strong convergence). By mid-October, my wife and I will put ChefBot to the test in a real dinner rush. Follow along to see if it outcooks GPT-4!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://brianhengen.us/tags/ai/>AI</a></li><li><a href=https://brianhengen.us/tags/slm/>SLM</a></li><li><a href=https://brianhengen.us/tags/fine-tuning/>Fine-Tuning</a></li><li><a href=https://brianhengen.us/tags/qwen/>Qwen</a></li><li><a href=https://brianhengen.us/tags/chefbot/>ChefBot</a></li><li><a href=https://brianhengen.us/tags/recipenlg/>RecipeNLG</a></li><li><a href=https://brianhengen.us/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://brianhengen.us/posts/fine-tuning-management-ai/><span class=title>¬´ Prev</span><br><span>Fine-Tuning a Personal Executive Assistant: Lessons from My Management Notes</span>
</a><a class=next href=https://brianhengen.us/posts/rtx-5090-build-process/><span class=title>Next ¬ª</span><br><span>Building the RTX 5090 AI Workstation - Step by Step</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on x" href="https://x.com/intent/tweet/?text=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f&amp;hashtags=AI%2cSLM%2cFine-tuning%2cQwen%2cChefBot%2cRecipeNLG%2cMachineLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f&amp;title=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice&amp;summary=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice&amp;source=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f&title=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on whatsapp" href="https://api.whatsapp.com/send?text=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice%20-%20https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on telegram" href="https://telegram.me/share/url?text=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice&amp;url=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Fine-Tuning Qwen Models: From Theory to Practice on ycombinator" href="https://news.ycombinator.com/submitlink?t=Fine-Tuning%20Qwen%20Models%3a%20From%20Theory%20to%20Practice&u=https%3a%2f%2fbrianhengen.us%2fposts%2ffine-tuning-qwen-models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://brianhengen.us/>Brian Hengen's AI Experiments</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><div id=mc_embed_shell style="max-width:600px;margin:2rem auto;padding:1.5rem;text-align:center;border-top:1px solid var(--border)"><div id=mc_embed_signup><form action="https://brianhengen.us18.list-manage.com/subscribe/post?u=6873340033b37c32cd7ea9c63&amp;id=ec4a1dcc46&amp;f_id=0077b2e6f0" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class=validate target=_blank><div id=mc_embed_signup_scroll><h3 style=margin-bottom:.5rem>Subscribe to New Posts</h3><p style=margin-bottom:1rem;font-size:.9rem>Get notified when I publish new articles about AI/ML training and workstation builds.</p><div class=mc-field-group style=margin-bottom:1rem><input type=email name=EMAIL class="required email" id=mce-EMAIL required placeholder="Your email address" style="padding:.5rem;width:100%;max-width:300px;border:1px solid var(--border);border-radius:4px;background:var(--entry);color:var(--primary)"></div><div id=mce-responses class=clear><div class=response id=mce-error-response style=display:none></div><div class=response id=mce-success-response style=display:none></div></div><div aria-hidden=true style=position:absolute;left:-5e3px><input type=text name=b_6873340033b37c32cd7ea9c63_ec4a1dcc46 tabindex=-1></div><div class=clear><input type=submit name=subscribe id=mc-embedded-subscribe class=button value=Subscribe style="padding:.5rem 2rem;background:var(--theme);color:var(--entry);border:none;border-radius:4px;cursor:pointer;font-weight:600"></div></div></form></div></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>