---
title: "Three LLMs, One App: Testing Fine-Tuned Models in Production"
date: 2025-10-09
draft: false
tags: ["AI", "LLM", "Performance", "Architecture", "Node.js", "Flutter", "Fine-tuning", "Deployment", "Oracle Database 23ai"]
categories: ["AI Infrastructure"]
description: "Comparing LLMs in the cloud (OCI GenAI), on a laptop (M2 Mac), and on a GPU server (RTX 5090) for a knowledge management app using Oracle Database 23ai. Spoiler: My laptop needs an upgrade!"
---

I spent a weekend fine-tuning a model for my knowledge management app, designed to handle my notes, PDFs, and presentations with Oracle Database 23ai's vector search. The model aced local testing on my RTX 5090 server, but what happened when I moved it to my M2 MacBook Pro for real-world use? It worked... barely. A query like "Summarize last week's customer meetings and identify risks" took over a minute, leaving me staring at a spinning wheel while my coffee got cold.

This is the story of how I built a 3-way LLM toggle to choose between cloud APIs, local models, and self-hosted GPU models—and why flexibility is key when the "best" model isn't always the right one.

## The Problem: One Size Doesn't Fit All

My app queries hundreds of documents—meeting notes, PDFs, spreadsheets—stored in Oracle Database 23ai, with vectors generated by OCI's Embedding service. Initially, I used Oracle's OCI GenAI service, which was fast and reliable. But my fine-tuned model, trained on years of management notes, understood my context better.

The challenge? Balancing competing priorities:
- **Security**: Keep sensitive data on-device
- **Power**: Get the best, most tailored answers
- **Speed**: Avoid waiting minutes for responses

No single LLM deployment could deliver everything. So I built a system to switch between three options, letting me choose based on the moment's needs—whether I'm on a plane or in the office.

## The Contenders

### Option 1: OCI GenAI (Cloud)
- **Model**: Cohere Command R+ (via Oracle Cloud)
- **Location**: us-chicago-1 region
- **Context**: 80K tokens

### Option 2: Fine-tuned Granite on M2 Mac (Local)
- **Model**: IBM Granite 4.0 H Micro (3B params, fine-tuned)
- **Location**: MacBook Pro M2 Max (64GB RAM)
- **Context**: 1.5K tokens (limited by buffer bug)

### Option 3: Granite on RTX 5090 (Self-hosted)
- **Model**: Granite 4.0 H Tiny (1-2B params)
- **Location**: Ubuntu server with RTX 5090
- **Context**: 8K tokens
- **Note**: Smaller model may hallucinate on specific details (e.g., customer names)

## The Performance Reality Check

Here's how the three performed on a typical query ("Summarize last week's customer meetings and identify risks"):

```chartjs
{
  "type": "bar",
  "data": {
    "labels": ["OCI GenAI", "M2 Mac (Granite)", "RTX 5090 (Granite)"],
    "datasets": [
      {
        "label": "Total Response (s)",
        "data": [3.5, 60, 2],
        "backgroundColor": "#4e79a7"
      },
      {
        "label": "Tokens/sec",
        "data": [50, 3, 40],
        "backgroundColor": "#f28e2b"
      }
    ]
  },
  "options": {
    "scales": {
      "y": {
        "beginAtZero": true
      }
    },
    "plugins": {
      "legend": {
        "position": "top"
      },
      "title": {
        "display": true,
        "text": "LLM Performance Comparison"
      }
    }
  }
}
```

| Metric | OCI GenAI | M2 Mac (Granite) | RTX 5090 (Granite) |
|--------|-----------|------------------|-------------------|
| **First Token** | 0.8s | 12s | 1s |
| **Total Response** | 3.5s | 60-120s | 1-2s |
| **Tokens/sec** | ~50 | ~3 | ~40 |
| **Memory Used** | 0 (cloud) | 8GB | 10GB VRAM |
| **Quality Score** | 8/10 | 9/10 | 6/10 |

The fine-tuned Granite on my M2 Mac gave the **best answers**, capturing my management style perfectly—but it was painfully slow, even with Oracle Database 23ai's efficient vector search. The RTX 5090 with Granite H Tiny was fast but less accurate, sometimes mixing up details like customer names. OCI GenAI hit the sweet spot for speed and reliability.

## Why Was the M2 Mac So Slow?

The Granite model uses a hybrid Mamba2 architecture, which is efficient for training but struggled on my M2 Mac due to:

1. **No GPU acceleration**: Running on CPU cores, despite Apple Silicon's MPS
2. **Memory bandwidth**: M2 Max has 400GB/s vs RTX 5090's 1TB/s
3. **Quantization overhead**: 4-bit quantization saves memory but slows compute
4. **Framework overhead**: llama.cpp isn't optimized for Mamba2
5. **Buffer bug**: A miscalculation in `api_server.py` requests excessive RAM (18-104GB) for prompts >1.5K, limiting context to 1.5K tokens

```python
# Expected (from training benchmarks)
inference_time = model_size / compute_power
# 3B params / Apple Neural Engine = fast!

# Actual
inference_time = (model_size * architecture_penalty * quantization_overhead) / cpu_cores
# Result: 15x slower than expected
```

## Building the 3-Way Toggle

I built a provider system to switch between LLMs, integrated with Oracle Database 23ai for data and vector storage. Here's the implementation:

### Backend Architecture (Node.js)

```javascript
// services/llmProviderManager.js
class LLMProviderManager {
  constructor() {
    this.providers = {
      oci: new OCIProvider(),
      local: new LocalGraniteProvider(),
      gpu: new GPUGraniteProvider()
    };
    this.defaultProvider = 'oci';
  }

  async askQuestion(question, context, provider = null) {
    const selectedProvider = provider || this.defaultProvider;
    const startTime = Date.now();
    const memBefore = process.memoryUsage();

    try {
      const response = await this.providers[selectedProvider].generate({
        prompt: this.buildPrompt(question, context),
        maxTokens: selectedProvider === 'local' ? 300 : 1000,
        temperature: 0.7
      });

      await this.logMetrics({
        provider: selectedProvider,
        responseTime: Date.now() - startTime,
        tokensGenerated: response.tokenCount,
        memoryUsed: process.memoryUsage().heapUsed - memBefore.heapUsed
      });

      return response;
    } catch (error) {
      if (selectedProvider !== this.defaultProvider) {
        console.log(`Provider ${selectedProvider} failed, falling back to OCI`);
        return this.askQuestion(question, context, this.defaultProvider);
      }
      throw error;
    }
  }
}
```

### Provider Implementations

```javascript
// providers/ociProvider.js
class OCIProvider {
  async generate({ prompt, maxTokens }) {
    const response = await axios.post(
      'https://genai.us-chicago-1.oci.oraclecloud.com/v1/chat',
      {
        model: 'cohere.command-r-plus',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: maxTokens
      },
      {
        headers: {
          'Authorization': `Bearer ${this.getOCIToken()}`,
          'Content-Type': 'application/json'
        }
      }
    );
    return {
      text: response.data.choices[0].message.content,
      tokenCount: response.data.usage.completion_tokens,
      provider: 'oci'
    };
  }
}

// providers/localGraniteProvider.js
class LocalGraniteProvider {
  async generate({ prompt, maxTokens }) {
    const response = await axios.post('http://localhost:11434/api/generate', {
      model: 'granite-manager:3b',
      prompt: prompt,
      stream: false,
      options: {
        num_predict: maxTokens,
        temperature: 0.7
      }
    });
    return {
      text: response.data.response,
      tokenCount: response.data.eval_count,
      provider: 'local'
    };
  }
}

// providers/gpuGraniteProvider.js
class GPUGraniteProvider {
  async generate({ prompt, maxTokens }) {
    const response = await axios.post('http://gpu-server:8000/v1/chat/completions', {
      model: 'granite-4.0-h-tiny',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: maxTokens,
      temperature: 0.7
    });
    return {
      text: response.data.choices[0].message.content,
      tokenCount: response.data.usage.completion_tokens,
      provider: 'gpu'
    };
  }
}
```

## Lessons Learned

### 1. Data Quality Over Hardware
My M2 Mac's slowness wasn't just hardware—it was exacerbated by a buffer bug in `api_server.py` that limited context to 1.5K tokens. Quality data (from my fine-tuning, see [previous post](https://brianhengen.us/posts/fine-tuning-management-ai/)) shone through, but only with **a lot of** patience.

### 2. Measure Everything
Tracking metrics like response time and token count was crucial:

```javascript
// middleware/metricsMiddleware.js
app.use('/api/chat', async (req, res, next) => {
  const metrics = {
    provider: req.body.provider,
    startTime: Date.now(),
    startMemory: process.memoryUsage()
  };
  const originalJson = res.json;
  res.json = function(data) {
    metrics.endTime = Date.now();
    metrics.duration = metrics.endTime - metrics.startTime;
    metrics.tokenCount = data.tokenCount || 0;
    metrics.tokensPerSecond = metrics.tokenCount / (metrics.duration / 1000);
    saveMetrics(metrics);
    res.set('X-LLM-Provider', metrics.provider);
    res.set('X-Response-Time', metrics.duration);
    res.set('X-Tokens-Per-Sec', metrics.tokensPerSecond);
    return originalJson.call(this, data);
  };
  next();
});
```

### 3. Flexibility Is Key
The 3-way toggle lets me switch based on context—OCI for speed, local for privacy, GPU for balance. This flexibility is something you get when leveraging Oracle Database 23ai's vector search.


## The Verdict: Right Tool for the Right Job

There's no "best" LLM deployment—only the best for your context:
- **Cloud (OCI)**: Fast, reliable, perfect for most queries with Oracle Database 23ai
- **Local (M2)**: Ideal for privacy and tailored answers, but needs a beefier laptop
- **Self-hosted (RTX 5090)**: Fast and private, but less accurate due to smaller model

**Key insight**: Build for flexibility. By supporting multiple providers, I can adapt to my needs—whether I'm offline or need instant answers.

## What's Next?

I'm working on:
1. **Model distillation**: Making my fine-tuned Granite model faster
2. **ONNX embeddings in Oracle Database 23ai**: Moving embeddings to the database for efficiency
3. **Multimedia support**: Adding native image/PDF understanding (revisiting with new embedding strategy)

The future is about orchestrating multiple LLMs, balancing general-purpose models with specialized ones.

## Implementation Checklist

To build something similar:
- [ ] Set up provider abstraction layer
- [ ] Implement health checks for each provider
- [ ] Add metrics logging
- [ ] Build provider selection UI
- [ ] Create fallback logic
- [ ] Set up monitoring and alerts
- [ ] Document expected response times
- [ ] Test offline scenarios



What trade-offs have you faced with LLM deployment? Have you tried integrating LLMs with Oracle Database 23ai? Drop a comment or reach out on GitHub! (See also my [ChefBot post](https://brianhengen.us/posts/fine-tuning-qwen-models) for more on specialized models.)

---

**Project Stats:**
- **Implementation time**: 3 days
- **Providers tested**: 3
