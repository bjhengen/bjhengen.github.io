---
title: "Fine-Tuning Qwen Models: From Theory to Practice"
date: 2025-09-29
draft: false
tags: ["AI", "SLM", "Fine-tuning", "Qwen", "ChefBot", "RecipeNLG", "Machine Learning"]
categories: ["AI Training"]
series: ["SLM Research"]
description: "Fine-tuning Qwen 32B and 14B models to create ChefBotâ€”a cooking AI that outperforms general-purpose LLMs in the kitchen."
---

Imagine an AI that coordinates your entire cooking processâ€”faster, smarter, and without ChatGPTâ€™s API costs. With my RTX 5090 workstation humming, Iâ€™m answering: **Can a specialized language model outcook ChatGPT in the kitchen?**

Over the past few days, Iâ€™ve been fine-tuning Qwenâ€™s **32B** and **14B** parameter models to create **ChefBot**, an experimental Specialized Language Model (SLM). Think ChatGPT for recipes, but tuned for cooking data and running without per-token API costs.

## Why ChefBot?

Cooking is collaborative, messy, and highly domain-specific. Picture cooking a three-course meal for friends, juggling prep times, oven space, and tasks for your novice sous-chef. General-purpose models like ChatGPT can suggest recipes, but they often miss the nuance of kitchen chaos. Thatâ€™s why Iâ€™m building **ChefBot** to:
- Master ingredient prep timing (e.g., chop onions while the oven preheats)
- Sequence cooking methods (e.g., sear steak before roasting)
- Coordinate equipment (e.g., use one pan for multiple dishes)
- Allocate tasks by skill (e.g., simple chopping for beginners)

A specialized model should handle this **better, faster, and cheaper** than a general-purpose LLM.

## Can the RTX 5090 Really Handle It?

The RTX 5090â€™s **32GB of VRAM** is a game-changer for training specialized models. Hereâ€™s how itâ€™s performing:

### Qwen-32B
- âœ… Completed 1000+ step training cycle
- ğŸ“‰ Training loss: 1.18 â†’ 0.68 (42% reduction)
- ğŸ§  Memory use: Fits comfortably in 32GB
- â±ï¸ Time: Several hours per cycle

### Qwen-14B
- â³ Currently at step 5,500 / 150,000
- ğŸ“‰ Eval loss: 1.0 â†’ 0.018
- âš¡ Speed: ~2â€“3 min per 50 steps (post-optimization)
- ğŸ§  Memory use: ~21GB

> *That 0.018 eval loss? That's like a Michelin star for a model.*

| Model     | VRAM Use | Training Speed       | Eval Loss | Practicality                  |
|-----------|----------|---------------------|-----------|-------------------------------|
| Qwen-32B  | 32GB     | Hours / 1k steps    | 0.68      | Powerful but resource-heavy    |
| Qwen-14B  | 21GB     | 2â€“3 min / 50 steps  | 0.018     | Ideal for speed and deployment |

The 32B model is impressive, but the 14B model is proving more practical: faster to train, easier to host, and surprisingly close in quality.

## Training Lessons

### ğŸ”§ Configuration Matters
Early runs were bottlenecked by CPU preprocessingâ€”painfully slow! The fix: crank up parallelism to leverage multi-core CPUs.

```yaml
# Before (single-threaded)
num_proc: 4
dataloader_num_workers: 0

# After (multi-core optimization)
num_proc: 10
dataloader_num_workers: 8
```

Result: CPU utilization matched the hardware, boosting throughput by ~30%. **Pro tip:** Check your CPU core count (e.g., `lscpu` on Linux) and set `num_proc` to match or slightly exceed it for max efficiency.

### ğŸ’¾ Checkpointing is Survival
Weekend-long training needs resilience:

```yaml
save_steps: 50                # checkpoint every ~2â€“5 min
save_total_limit: 3           # keep only the last 3
resume_from_checkpoint: true  # auto-resume after interruption
```

This lets me pause/restart training without losing progress.

## Real-World Training Data

Iâ€™m training on the [RecipeNLG dataset](https://arxiv.org/abs/2004.10409) (~2.2M recipes from AllRecipes, Food.com, and more), packed with detailed cooking instructions, ingredient lists, and techniques. ChefBot is learning:
- Ingredient preparation timing
- Cooking method sequences
- Equipment coordination
- Skill-based task allocation

The eval loss drop shows itâ€™s absorbing real cooking knowledge.

## Whatâ€™s Next

The 14B model is grinding through a 3-epoch (150k step) cycle. Once complete, Iâ€™ll:
1. **Benchmark against GPT-4** in a head-to-head recipe showdown
2. **Test it in my kitchen** with my wife, cooking a full meal under pressure
3. **Optimize for deployment** using quantization for lighter, faster hosting

Got a favorite recipe for ChefBot to tackle? Drop it in the comments or tweet me at @bhengen!

## The Business Case

Specialized models like ChefBot deliver serious ROI over giant LLMs:
- ğŸ’° **No API costs**â€”just ~$50/month hosting vs. $100s in API fees
- ğŸ³ **Superior cooking knowledge**â€”tuned on 2.2M recipes
- ğŸ”’ **Enhanced privacy**â€”recipes stay local, no cloud leaks
- ğŸ› ï¸ **Full customization**â€”I control updates, no waiting for OpenAI

This project proves smaller, specialized models can beat giants when context matters.

## Stay Tuned

Training continues (step 17,500+ with strong convergence). By mid-October, my wife and I will put ChefBot to the test in a real dinner rush. Follow along to see if it outcooks GPT-4!